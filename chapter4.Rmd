---
title: "Chapter 4"
output: html_document
date: "2022-11-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the boston data and checking the structure

```{r}
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)
dim(Boston)
```


Dataset has 506 observations and 14 variables. All variables are numerical and each one describes housing values in Boston. More info on the variables can be found from [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). 


## Graphical overview of the data

```{r}
# plot matrix of the variables
library(ggplot2)
library("GGally")
ggpairs(Boston[1:8])
```


```{r}

ggpairs(Boston[8:14])
```

There are plenty of variables in the data that correlate significantly as can be seen from the Pearson correlation coefficients that are displayed on the right hand side of the plot. Next, I will use a correlation matrix to get a better view and to be able to distinguish the variables that stand up from the data. 

```{r}
cor_matrix <- cor(Boston) 
cor_matrix <- round(cor_matrix, digits = 2)

# rounded correlation matrix
cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")

```

Correlation plot of the variables in the Boston data

High positive correlation: 

Tax (full value property tax-rate per 10 000 dollars) and rad (index of accessibility to radial highways) 0.910 \*\*\*. 

Highest negative correlations: 

Lstat (lower status of the populaitons) and medv (median value of owner occupied houses in 1000$s) -0.738\*\*\*

Age (proportion of owner occupied units build before 1940) and dis (weighed mean of distances to five boston employment centers) -0.708\*\*\*, 

Indus (proportion of non retail business acres per town) and dis (weighed mean of distances to five boston employment centers) -0.708\*\*\*. 

## Standardize the dataset and print out summaries of the scaled data
Scaling means that the column means are first subtracted from the columns and then divided by the standard deviation. Scaling will shift the mean to 0 as we can see here. 

```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
summary(Boston)
# change into a data frame for future use
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)

```

## Turn crime rate into categorical variable, use quantiles as the break points and replace the old crime rate variable with the new one

```{r}

summary(boston_scaled$crim)

# creating a quantile vector
bins <- quantile(boston_scaled$crim)
bins

# next a categorical variable
labelsadd <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = labelsadd)
# how does it look like?
table(crime)

# Replacing the old crim with the new catergorical crim
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)


```

## Divide the dataset into train and test sets (80% to the train set)

```{r}
# number of rows
nrow(boston_scaled)
n <- 506

# randomly select 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create the train set
train <- boston_scaled[ind,]

# and the test set
test <- boston_scaled[-ind,]

# correct classes from the test data 
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```


## Fit the linear discriminant analysis on the train set using the categorical crime rate as the target and all other variables as predictors and draw the LDA plot

```{r}
library(MASS)

# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)
lda.fit

# biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# LDA plot
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

##  Predict the classes with the LDA model on the test data and cross tabulate the results with crime categories from the test set

Crime categories have already been saved from the test set and categorical crime variable has been removed from the test dataset. 

```{r}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)


```

Predicted vs. correct results interpretation: Classifier predicted the crime rates well in the case of high crime rates.  Low and med_high had most inaccurately predicted instances. both of these categories had over 40% error rates. 



## Reload and standardize the Boston dataset
```{r}
data("Boston")
dim(Boston)

boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```

## Calculate the distances between the observations

```{r}
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)
```

## Run k-means algorithm on the dataset

```{r}
library(tidyverse)
# k-means 
km <- kmeans(Boston, centers = 4)

```


## Optimal number of clusters and visualization of the clusters

This is where I ran into an issue. I can't make the exercise to work. If I try to run the code to calculate the total within sum of squares, the function
 twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss}) produces following error: 
 "Warning: numerical expression has 9 elements: only the first usedError in 1:k_max : NA/NaN argument."